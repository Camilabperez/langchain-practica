{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4759bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca00dd0",
   "metadata": {},
   "source": [
    "Gemini \n",
    "- 2.0 flash gratuita pero limitada\n",
    "anthropic(Claude) - gratuita pero limitada\n",
    "- \n",
    "OpenAI api paga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766ba15",
   "metadata": {},
   "source": [
    "# Componentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996be06",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6457805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about Python.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "promt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adejective} joke about {topic}.\"\n",
    ")\n",
    "\n",
    "promt_template.format(\n",
    "    adejective=\"funny\",\n",
    "    topic=\"Python\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdfe1d",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d66c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the Python say goodbye to the C++ programmer?\\n\\nBecause he didn't want to get bitten by a class!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = promt | llm\n",
    "chain.invoke({\"input\": \"Tell me a funny joke about Python.\"}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700e401",
   "metadata": {},
   "source": [
    "Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a242d1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a message to escalate positive feedback to a human agent, keeping in mind the \"positive\" aspect:\n",
      "\n",
      "**Option 1 (Short & Sweet):**\n",
      "\n",
      "> \"Escalating to a human agent to share positive feedback received. Please review and consider for recognition/further action.\"\n",
      "\n",
      "**Option 2 (Slightly More Detailed):**\n",
      "\n",
      "> \"Routing this positive feedback to a human agent for review.  This feedback highlights [mention a specific area if possible, e.g., excellent customer service, product innovation, etc.].  Please assess for potential rewards, recognition, or further action.\"\n",
      "\n",
      "**Option 3 (Focus on Impact):**\n",
      "\n",
      "> \"Escalating this exceptionally positive feedback to a human agent. This feedback could be valuable for [mention potential use, e.g., employee performance reviews, marketing materials, product development]. Please review and determine appropriate next steps.\"\n",
      "\n",
      "**Key Considerations When Choosing:**\n",
      "\n",
      "*   **Your Internal Process:** Does your company have a specific process for handling positive feedback? The message should align with that.\n",
      "*   **Level of Detail:** Can you quickly summarize the feedback's key point? Adding that detail helps the agent understand the context immediately.\n",
      "*   **Desired Outcome:** What do you *want* to happen with the feedback? (Recognition, reward, etc.)  Hinting at the desired outcome can be helpful.\n",
      "\n",
      "**Important:**  Make sure the actual feedback is included along with the escalation message! The agent needs to see the original feedback to understand why it's being escalated.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# Define prompt templates for different feedback types\n",
    "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the feedback classification template\n",
    "classification_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the runnable branches for handling feedback\n",
    "branches = RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x,\n",
    "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x,\n",
    "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x,\n",
    "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
    "    ),\n",
    "    escalate_feedback_template | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create the classification chain\n",
    "classification_chain = classification_template | model | StrOutputParser()\n",
    "\n",
    "# Combine classification and response generation into one chain\n",
    "chain = classification_chain | branches\n",
    "\n",
    "# Run the chain with an example review\n",
    "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
    "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
    "\n",
    "review = \"he product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "\n",
    "# Output the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd0ee9",
   "metadata": {},
   "source": [
    "## Memoria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41272fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Eres un asistente especializado en {abillity}. Puedes responder preguntas sobre {abillity}\"),  # Changed from set to tuple\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),  # Changed from set to tuple\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bdb45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable=runnable,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b4b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Excelente elección! La ciencia de datos es un campo fascinante y en constante crecimiento. Para ayudarte a empezar, te propongo una hoja de ruta estructurada y algunos recursos clave.\\n\\n**1. Fundamentos:**\\n\\n*   **Matemáticas:**\\n    *   **Álgebra Lineal:** Vectores, matrices, operaciones matriciales, autovalores y autovectores. Esencial para entender algoritmos de machine learning.\\n    *   **Cálculo:** Derivadas, integrales, optimización. Importante para entender cómo funcionan los algoritmos de optimización en machine learning.\\n    *   **Estadística y Probabilidad:** Estadística descriptiva, distribuciones de probabilidad, pruebas de hipótesis, intervalos de confianza. Fundamental para el análisis de datos y la inferencia estadística.\\n*   **Programación:**\\n    *   **Python:** El lenguaje más popular en ciencia de datos. Aprende la sintaxis básica, estructuras de datos, funciones, clases y módulos.\\n    *   **R:** Otro lenguaje popular, especialmente en estadística.\\n*   **Bases de Datos:**\\n    *   **SQL:** Para consultar y manipular datos en bases de datos relacionales.\\n    *   **NoSQL (opcional):** MongoDB, Cassandra, etc. Útiles para datos no estructurados o grandes volúmenes de datos.\\n\\n**2. Herramientas Clave:**\\n\\n*   **Bibliotecas de Python:**\\n    *   **NumPy:** Para computación numérica eficiente con arrays multidimensionales.\\n    *   **Pandas:** Para manipulación y análisis de datos con DataFrames.\\n    *   **Matplotlib y Seaborn:** Para visualización de datos.\\n    *   **Scikit-learn:** Para algoritmos de machine learning (clasificación, regresión, clustering, etc.).\\n    *   **TensorFlow y PyTorch:** Para deep learning (redes neuronales).\\n*   **Entornos de Desarrollo:**\\n    *   **Jupyter Notebook/Lab:** Para crear documentos interactivos que combinan código, texto y visualizaciones.\\n    *   **Google Colab:** Un entorno Jupyter Notebook en la nube, ideal para empezar sin necesidad de instalar nada.\\n    *   **VS Code:** Un editor de código potente con extensiones para Python y R.\\n\\n**3. Aprendizaje de Machine Learning:**\\n\\n*   **Conceptos Básicos:**\\n    *   **Aprendizaje Supervisado:** Clasificación, regresión.\\n    *   **Aprendizaje No Supervisado:** Clustering, reducción de dimensionalidad.\\n    *   **Evaluación de Modelos:** Métricas de rendimiento, validación cruzada.\\n    *   **Overfitting y Underfitting:** Cómo evitar estos problemas.\\n*   **Algoritmos:**\\n    *   **Regresión Lineal y Logística**\\n    *   **Árboles de Decisión y Random Forests**\\n    *   **Máquinas de Vectores de Soporte (SVM)**\\n    *   **K-Means Clustering**\\n    *   **Redes Neuronales (conceptos básicos)**\\n\\n**4. Proceso de Ciencia de Datos:**\\n\\n1.  **Recopilación de Datos:** Obtener datos de diversas fuentes (bases de datos, APIs, archivos, etc.).\\n2.  **Limpieza y Preparación de Datos:** Manejar valores faltantes, eliminar duplicados, transformar datos.\\n3.  **Análisis Exploratorio de Datos (EDA):** Visualizar datos, calcular estadísticas descriptivas, identificar patrones y relaciones.\\n4.  **Modelado:** Seleccionar y entrenar modelos de machine learning.\\n5.  **Evaluación:** Evaluar el rendimiento del modelo y ajustarlo si es necesario.\\n6.  **Implementación:** Desplegar el modelo para que pueda ser utilizado en producción.\\n7.  **Comunicación:** Comunicar los resultados y las conclusiones de manera clara y efectiva.\\n\\n**5. Recursos de Aprendizaje:**\\n\\n*   **Cursos Online:**\\n    *   **Coursera:** \"Machine Learning\" de Andrew Ng (Stanford), \"Data Science Specialization\" de Johns Hopkins University.\\n    *   **edX:** \"Data Science Professional Certificate\" de HarvardX.\\n    *   **Udacity:** \"Data Science Nanodegree\".\\n    *   **DataCamp:** Cursos interactivos de Python, R, SQL y machine learning.\\n    *   **Platzi:** Carrera de Ciencia de Datos.\\n*   **Libros:**\\n    *   \"Python Data Science Handbook\" de Jake VanderPlas.\\n    *   \"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\" de Aurélien Géron.\\n    *   \"The Elements of Statistical Learning\" de Hastie, Tibshirani y Friedman (disponible gratuitamente online).\\n*   **Tutoriales y Documentación:**\\n    *   Documentación oficial de NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch.\\n    *   Tutoriales de Kaggle.\\n    *   Blogs de ciencia de datos (Towards Data Science, Analytics Vidhya).\\n*   **Comunidades:**\\n    *   Kaggle: Para participar en competiciones de ciencia de datos y aprender de otros.\\n    *   Stack Overflow: Para hacer preguntas y obtener respuestas sobre problemas de programación y ciencia de datos.\\n    *   Grupos de Meetup locales: Para conectar con otros científicos de datos en tu área.\\n\\n**6. Proyectos Prácticos:**\\n\\n*   **Empieza con proyectos pequeños:**\\n    *   Analizar un conjunto de datos público (por ejemplo, datos de COVID-19, datos de calidad del aire).\\n    *   Construir un modelo de clasificación simple (por ejemplo, clasificar correos electrónicos como spam o no spam).\\n    *   Crear una visualización de datos interactiva.\\n*   **Participa en competiciones de Kaggle:** Es una excelente manera de aprender y practicar tus habilidades.\\n*   **Crea tu propio portafolio:** Muestra tus proyectos en GitHub o en un sitio web personal.\\n\\n**Consejos Adicionales:**\\n\\n*   **Sé constante:** Dedica tiempo regularmente al aprendizaje.\\n*   **Aprende haciendo:** La mejor manera de aprender es practicar.\\n*   **No tengas miedo de pedir ayuda:** La comunidad de ciencia de datos es muy colaborativa.\\n*   **Mantente actualizado:** La ciencia de datos está en constante evolución, así que sigue aprendiendo nuevas técnicas y herramientas.\\n*   **Enfócate en un área específica:** Una vez que tengas una base sólida, puedes especializarte en un área como procesamiento del lenguaje natural (NLP), visión por computadora, análisis de series temporales, etc.\\n\\n¡Mucha suerte en tu viaje de aprendizaje en ciencia de datos! Si tienes alguna pregunta específica, no dudes en preguntar.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "        {\"abillity\":\"ciencia de datos\", \"input\":\"Quero aprender ciencia de dados.\"},\n",
    "        config={\"configurable\": {\"session_id\": \"12345\"}},\n",
    ").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e3b8078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para que pueda ayudarte de la mejor manera, necesito que me especifiques un poco más tu pregunta.  ¿A qué te refieres con \"¿Cómo?\"?  Podría ser:\\n\\n*   **¿Cómo empiezo a aprender ciencia de datos?** (Si necesitas una guía más detallada sobre los primeros pasos).\\n*   **¿Cómo instalo Python y las bibliotecas necesarias?** (Si necesitas ayuda con la configuración del entorno).\\n*   **¿Cómo encuentro conjuntos de datos para practicar?** (Si necesitas recursos para encontrar datos).\\n*   **¿Cómo abordo un proyecto de ciencia de datos?** (Si necesitas una guía sobre el flujo de trabajo).\\n*   **¿Cómo aprendo un tema específico (por ejemplo, álgebra lineal, estadística, machine learning)?** (Si necesitas recursos para un tema en particular).\\n\\nCuanto más específica sea tu pregunta, mejor podré ayudarte.  ¡No dudes en darme más detalles!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "        {\"abillity\":\"ciencia de datos\", \"input\":\"Como?\"},\n",
    "        config={\"configurable\": {\"session_id\": \"12345\"}},\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645d20a",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b61610",
   "metadata": {},
   "source": [
    "Solo tiene procesado hasta lo que se entreno, por lo que se pueden usar tools para buscar informacion en otras fuentes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b5fcf",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7d7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de herramienta: wikipedia\n",
      "Descripción: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Argumentos: {'query': {'description': 'query to look up on wikipedia', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "print(\"Nombre de herramienta:\", wiki_tool.name) \n",
    "print(\"Descripción:\", wiki_tool.description) \n",
    "print(\"Argumentos:\", wiki_tool.args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59a93d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n\\nPage: Milvus (vector database)\\nSummary: Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\\nMilvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\\n\\nPage: Intelligent agent\\nSummary: In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tool.run(\"LangChain\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803e3b8",
   "metadata": {},
   "source": [
    "### Servicio Tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43efc9",
   "metadata": {},
   "source": [
    "https://app.tavily.com/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0573f19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Socius\\AppData\\Local\\Temp\\ipykernel_5928\\3392196689.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  searchTavily = TavilySearchResults()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': '¿Qué es LangChain? - Google Cloud',\n",
       "  'url': 'https://cloud.google.com/use-cases/langchain?hl=es-419',\n",
       "  'content': '# ¿Qué es LangChain?\\n\\nLangChain es una plataforma de lenguaje de programación que permite a los desarrolladores construir y conectar modelos para acceder, transformar y compartir datos sin problemas. Ofrece un entorno potente y versátil para el desarrollo de modelos, lo que permite la manipulación de estructuras de datos, la encadenación de modelos y la incorporación de agentes externos como LaMDA. [...] En su núcleo, LangChain aprovecha una arquitectura distribuida que permite un procesamiento eficiente y escalable de datos de lenguaje. Emplea un diseño basado en microservicios, en el que cada cadena se ejecuta como un servicio independiente, lo que facilita la implementación y administración flexibles. Esta arquitectura permite una integración continua con servicios externos, incluidos los LLM y las fuentes de datos basadas en la nube. [...] LangChain funciona según el principio de la modularidad, que descompone los sistemas de IA basados en el lenguaje en componentes reutilizables. Estos componentes, conocidos como “cadenas”, encapsulan funcionalidades específicas, como la recuperación de datos, la interacción del modelo y la administración de la memoria. Al ensamblar estas cadenas en varias configuraciones, los desarrolladores pueden adaptar LangChain para cumplir con los requisitos únicos de sus aplicaciones.',\n",
       "  'score': 0.9537642},\n",
       " {'title': '¿Qué es LangChain? - IBM',\n",
       "  'url': 'https://www.ibm.com/es-es/think/topics/langchain',\n",
       "  'content': 'LangChain es un marco de orquestación de código abierto para el desarrollo de aplicaciones que utilizan [modelos de lenguaje de gran tamaño (LLM)](https://www.ibm.com/es-es/think/topics/open-source-llms). Disponibles en bibliotecas basadas en Python y Javascript, las herramientas y API de LangChain simplifican el proceso de creación de aplicaciones impulsadas por LLM, como [chatbots](https://www.ibm.com/es-es/topics/chatbots) y [agentes [...] LangChain es esencialmente una biblioteca de abstracciones para Python y Javascript, que representa los pasos y conceptos comunes necesarios para trabajar con modelos de lenguaje. Estos componentes modulares, como las funciones y las clases de objetos, sirven como bloques de construcción de los programas de IA generativa. Se pueden \"*encadenar*\" entre sí para crear aplicaciones, minimizando la cantidad de código y la comprensión precisa necesarios para ejecutar tareas complejas de PLN. Aunque [...] Descubra ideas y noticias de expertos sobre IA, nube y mucho más en el boletín semanal Think.\\n\\n## ¿Cómo funciona LangChain?\\n\\nEl núcleo de LangChain es un entorno de desarrollo que racionaliza la programación de aplicaciones LLM mediante el uso de\\xa0*abstracción*: la simplificación del código representando uno o más procesos complejos como un componente con nombre que encapsula todos sus pasos constituyentes.',\n",
       "  'score': 0.94339466},\n",
       " {'title': 'Introducción a LangChain y sus capacidades - OpenWebinars',\n",
       "  'url': 'https://openwebinars.net/blog/introduccion-langchain/',\n",
       "  'content': 'LangChain es un marco de trabajo de código abierto diseñado para simplificar la creación de aplicaciones utilizando modelos de lenguaje grande (',\n",
       "  'score': 0.94276565},\n",
       " {'title': '¿Qué es LangChain? - IBM',\n",
       "  'url': 'https://www.ibm.com/mx-es/think/topics/langchain',\n",
       "  'content': 'LangChain es un marco de orquestación de código abierto para desarrollar aplicaciones con [modelos de lenguaje de gran tamaño (LLM)](https://www.ibm.com/mx-es/think/topics/open-source-llms). Proporciona herramientas en Python y JavaScript para integrar LLMs, bases de datos y APIs, facilitando la creación de [chatbots](https://www.ibm.com/mx-es/topics/chatbots),\\xa0[agentes virtuales](https://www.ibm.com/mx-es/topics/virtual-agent) y sistemas de generación de texto. [...] LangChain es esencialmente una biblioteca de abstracciones para Python y Javascript, que representa los pasos y conceptos comunes necesarios para trabajar con modelos lingüísticos. Estos componentes modulares, como las funciones y las clases de objetos, sirven como bloques de construcción de los programas de IA generativa. Se pueden \"*encadenar* \" entre sí para crear aplicaciones, minimizando la cantidad de código y la comprensión precisa necesarios para ejecutar tareas complejas de PLN. Aunque [...] [Las herramientas de LangChain](https://python.langchain.com/v0.2/docs/integrations/tools/)\\xa0(enlace externo a ibm.com) son un conjunto de funciones que permiten a los agentes de LangChain interactuar con información del mundo real con el fin de ampliar o mejorar los servicios que puede proporcionar. Algunos ejemplos de herramientas destacadas de LangChain son:',\n",
       "  'score': 0.9403148},\n",
       " {'title': '¿Qué es LangChain? - AWS',\n",
       "  'url': 'https://aws.amazon.com/es/what-is/langchain/',\n",
       "  'content': 'LangChain es un marco de trabajo de código abierto para crear aplicaciones basadas en modelos de lenguaje de gran tamaño (LLM). Los LLM son grandes modelos de aprendizaje profundo entrenados previamente con grandes cantidades de datos que pueden generar respuestas a las consultas de los usuarios, por ejemplo, responder preguntas o crear imágenes a partir de peticiones basadas en texto. LangChain proporciona herramientas y abstracciones para mejorar la personalización, precisión y relevancia de [...] ### **Soporte para desarrolladores**\\n\\nLangChain le brinda herramientas a los desarrolladores de IA para conectar modelos de lenguaje con orígenes de datos externos. Es de código abierto y cuenta con el apoyo de una comunidad activa. Las organizaciones pueden usar LangChain de forma gratuita y recibir el apoyo de otros desarrolladores que dominen el marco.\\n\\n## ¿Cómo funciona LangChain? [...] Con Amazon Bedrock, Amazon Kendra, Amazon SageMaker JumpStart, LangChain y sus LLM, puede crear aplicaciones de inteligencia artificial generativa (IA generativa) de alta precisión a partir de datos empresariales. LangChain es la interfaz que une estos componentes:\\n\\nPara comenzar a trabajar con LangChain en AWS, [cree una cuenta](https://portal.aws.amazon.com/billing/signup) hoy mismo.\\n\\n## Pasos siguientes en AWS',\n",
       "  'score': 0.9368016}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "searchTavily = TavilySearchResults()\n",
    "searchTavily.invoke(\"Que es LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b245ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista de herramientas\n",
    "tools = [wiki_tool, searchTavily]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122ff4a",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75367d9d",
   "metadata": {},
   "source": [
    "Son agentes de software autnomas que interactuan y responder dinamicamente con los usuarios y el entorno\n",
    "- Capaces dde realizar acciones\n",
    "- Utilizan y orquestan chains\n",
    "- Mantiene contexto y memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c2556",
   "metadata": {},
   "source": [
    "HUB: Almacen de promts de la comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c9ad53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66966fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un asistente de chat amigable y servicial, especializado en ayudar con información sobre Azure DevOps. Tu principal tarea es responder preguntas sobre el contenido de la **Wiki del proyecto Prueba-MCP** de Azure DevOps. Por favor, responde siempre en **español** y mantén un tono **simpático y conversacional**. Para responder a las preguntas, utilizarás tus herramientas para **buscar en la Wiki de Azure DevOps**. Es fundamental que asumas que toda la información relevante para las preguntas de los usuarios se encuentra dentro del **proyecto Prueba-MCP** y puede estar en **cualquier página de su Wiki**. Cuando un usuario te pregunte algo, tu primer paso debe ser buscar la respuesta en la Wiki utilizando alguna de las herramientas de búsqueda de información que tengas disponible para la Wiki de Azure DevOps. Si la información no está directamente disponible en la Wiki o si necesitas más detalles, puedes solicitar aclaraciones al usuario o sugerirle que la información podría no estar documentada. Si el usuario te pide algo que no está relacionado con la Wiki de Azure DevOps o las herramientas que tienes para interactuar con Azure DevOps, hazle saber amablemente que tu función principal es ayudarte con la Wiki y Azure DevOps.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"Eres un asistente de chat amigable y servicial, especializado en ayudar con información sobre Azure DevOps. Tu principal tarea es responder preguntas sobre el contenido de la **Wiki del proyecto Prueba-MCP** de Azure DevOps. Por favor, responde siempre en **español** y mantén un tono **simpático y conversacional**. Para responder a las preguntas, utilizarás tus herramientas para **buscar en la Wiki de Azure DevOps**. Es fundamental que asumas que toda la información relevante para las preguntas de los usuarios se encuentra dentro del **proyecto Prueba-MCP** y puede estar en **cualquier página de su Wiki**. Cuando un usuario te pregunte algo, tu primer paso debe ser buscar la respuesta en la Wiki utilizando alguna de las herramientas de búsqueda de información que tengas disponible para la Wiki de Azure DevOps. Si la información no está directamente disponible en la Wiki o si necesitas más detalles, puedes solicitar aclaraciones al usuario o sugerirle que la información podría no estar documentada. Si el usuario te pide algo que no está relacionado con la Wiki de Azure DevOps o las herramientas que tienes para interactuar con Azure DevOps, hazle saber amablemente que tu función principal es ayudarte con la Wiki y Azure DevOps.\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40f97803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "promt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df74e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Eres un asistente de chat amigable y servicial, que responde a la pregunta del usuario: {input}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "prompt = prompt_template.invoke({\"input\": \"{input}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5e1a37b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChatPromptTemplate.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m prompt = \u001b[43mChatPromptTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplaceholder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{msgs}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <-- This is the changed part\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: ChatPromptTemplate.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\n",
    "],\n",
    "\n",
    "    (\"tools\", tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b818a55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Prompt missing required variables: {'tools', 'agent_scratchpad', 'tool_names'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_tool_calling_agent, create_react_agent, create_structured_chat_agent\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m agent = \u001b[43mcreate_structured_chat_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m agent_executor = AgentExecutor(\n\u001b[32m     11\u001b[39m     agent=agent,\n\u001b[32m     12\u001b[39m     tools=tools,\n\u001b[32m     13\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m agent_executor.invoke(\n\u001b[32m     17\u001b[39m     {\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHola!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain\\agents\\structured_chat\\base.py:281\u001b[39m, in \u001b[36mcreate_structured_chat_agent\u001b[39m\u001b[34m(llm, tools, prompt, tools_renderer, stop_sequence)\u001b[39m\n\u001b[32m    277\u001b[39m missing_vars = {\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtool_names\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33magent_scratchpad\u001b[39m\u001b[33m\"\u001b[39m}.difference(\n\u001b[32m    278\u001b[39m     prompt.input_variables + \u001b[38;5;28mlist\u001b[39m(prompt.partial_variables)\n\u001b[32m    279\u001b[39m )\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_vars:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrompt missing required variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    283\u001b[39m prompt = prompt.partial(\n\u001b[32m    284\u001b[39m     tools=tools_renderer(\u001b[38;5;28mlist\u001b[39m(tools)),\n\u001b[32m    285\u001b[39m     tool_names=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([t.name \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tools]),\n\u001b[32m    286\u001b[39m )\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_sequence:\n",
      "\u001b[31mValueError\u001b[39m: Prompt missing required variables: {'tools', 'agent_scratchpad', 'tool_names'}"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent, create_react_agent, create_structured_chat_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent = create_structured_chat_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Hola!\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'LangChain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Milvus (vector database)\n",
      "Summary: Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\n",
      "Milvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\n",
      "\n",
      "Page: Intelligent agent\n",
      "Summary: In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\n",
      "A specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\n",
      "Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\n",
      "Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm's behavior is guided by a fitness function.\n",
      "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
      "Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\u001b[0m\u001b[32;1m\u001b[1;3mLangChain es un marco de software que ayuda a facilitar la integración de grandes modelos de lenguaje (LLM) en aplicaciones. Sus casos de uso incluyen el análisis y resumen de documentos, chatbots y análisis de código.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Quiero aprender sobre LangChain.',\n",
       " 'output': 'LangChain es un marco de software que ayuda a facilitar la integración de grandes modelos de lenguaje (LLM) en aplicaciones. Sus casos de uso incluyen el análisis y resumen de documentos, chatbots y análisis de código.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando els ervicio de wikipedia\n",
    "agent_executor.invoke({\"input\": \"Quiero aprender sobre LangChain.\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf1304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'temperatura en buenos aires argentina'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Previsión meteorológica de tres días para Buenos Aires ...', 'url': 'https://www.accuweather.com/es/ar/buenos-aires/7894/weather-forecast/7894', 'content': 'Temperatura. Previsión por horas. 12 52°. rain drop 0 % · 13 54°. rain ... Internacional América del Sur Argentina Ciudad Autónoma de Buenos Aires Buenos Aires', 'score': 0.80070794}, {'title': 'Tiempo en Buenos Aires. Clima a 14 días', 'url': 'https://www.meteored.com.ar/tiempo-en_Buenos+Aires-America+Sur-Argentina-Ciudad+Autonoma+de+Buenos+Aires-SABE-1-13584.html', 'content': '|  | | * Lluvia  Lluvia  **0% 0 mm** * Humedad  Humedad **56%** * Punto de rocío  Punto de rocío  **4 °C** * Nubosidad  Nubosidad **28%**  * Sensación Térmica  Sensación Térmica  **13 °C** * Visibilidad  Visibilidad **50 km** * Viento  Viento - Medio **12 km/h** * Presión  Presión  **1020 hPa**  * Niebla  Niebla **No** * Viento - Rachas  Viento - Rachas  **26 km/h** * Cota Nieve  Cota Nieve **3200 m** | | | | | |  | [...] |  | | * Lluvia  Lluvia  **0% 0 mm** * Humedad  Humedad **72%** * Punto de rocío  Punto de rocío  **6 °C** * Nubosidad  Nubosidad **39%**  * Sensación Térmica  Sensación Térmica  **11 °C** * Visibilidad  Visibilidad **45 km** * Viento  Viento - Medio **13 km/h** * Presión  Presión  **1020 hPa**  * Niebla  Niebla **No** * Viento - Rachas  Viento - Rachas  **25 km/h** * Cota Nieve  Cota Nieve **3100 m** | | | | | |  | [...] |  | | * Lluvia  Lluvia  **0% 0 mm** * Humedad  Humedad **67%** * Punto de rocío  Punto de rocío  **5 °C** * Nubosidad  Nubosidad **35%**  * Sensación Térmica  Sensación Térmica  **11 °C** * Visibilidad  Visibilidad **45 km** * Viento  Viento - Medio **12 km/h** * Presión  Presión  **1020 hPa**  * Niebla  Niebla **No** * Viento - Rachas  Viento - Rachas  **22 km/h** * Cota Nieve  Cota Nieve **3200 m** | | | | | |  |', 'score': 0.599087}, {'title': 'El Tiempo en Buenos Aires. Predicción a 14 días - Meteored', 'url': 'https://www.tiempo.com/buenos-aires.htm', 'content': '|  | 17:00 | Parcialmente nuboso |  | 12° | **Parcialmente nuboso**    Sensación T. 12° | Calma  **Oeste**  9 - 25 km/h | Índice UV  **0 Bajo** FPS: no |  | [...] |  | 18:00 | Nubes y claros |  | 12° | **Nubes y claros**    Sensación T. 12° | Calma  **Oeste**  9 - 29 km/h | Índice UV  **0 Bajo** FPS: no |  | [...] |  | | * Lluvia  Lluvia  **0% 0 l/m²** * Humedad  Humedad **51%** * Punto de rocío  Punto de rocío  **3 °C** * Nubosidad  Nubosidad **50%**  * Sensación Térmica  Sensación Térmica  **13 °C** * Visibilidad  Visibilidad **50 km** * Viento  Viento - Medio **11 km/h** * Presión  Presión  **1013 hPa**  * Niebla  Niebla **No** * Viento - Rachas  Viento - Rachas  **29 km/h** * Cota Nieve  Cota Nieve **2800 m** | | | | | |  |', 'score': 0.5837973}, {'title': 'Clima en Buenos Aires hoy y pronóstico del tiempo a 14 días', 'url': 'https://www.clima.com/argentina/buenos-aires/buenos-aires', 'content': '4 de JUL\\nViernes a las 20:00\\n\\nViento\\n\\n7\\nkm/h\\n\\nRáfagas\\n\\n10\\nkm/h\\n\\nSensación térmica\\n\\n11°\\n\\nLluvia\\n\\n0 mm\\n\\nNieve\\n\\n0\\ncm\\n\\nNubes\\n\\n69%\\n\\nProb. de precipitación\\n\\n20%\\n\\nRadiación UV\\n\\n-\\n\\nProb. de Tormenta\\n\\n0%\\n\\nHumedad\\n\\n75%\\n\\nPresión\\n\\n1018 hPa\\n\\nAmanecer\\n\\n08:00\\n\\nAnochecer\\n\\n17:57\\n\\nFase Lunar\\n\\nGibosa Creciente\\n\\n16\\nkm/h\\n\\n11\\nkm/h\\n\\n9\\nkm/h\\n\\n13\\nkm/h\\n\\n20\\nkm/h\\n\\n10\\nkm/h\\n\\n10\\nkm/h\\n\\n12\\nkm/h\\n\\n12\\nkm/h\\n\\n11\\nkm/h\\n\\n12\\nkm/h\\n\\n12\\nkm/h\\n\\n11\\nkm/h\\n\\n10\\nkm/h\\n\\n[Inicio](/)\\n\\n[Argentina](/argentina)\\n\\n[Buenos Aires](/argentina/buenos-aires) [...] | 14:00 | Intervalos nubosos 13° 25 de JUN Hoy a las 14:00   * Viento  16   km/h * Ráfagas  24   km/h * Sensación térmica  12° * Lluvia  0 mm * Nieve  0   cm * Nubes  71% * Prob. de precipitación  20% * Radiación UV  2 Baja * Prob. de Tormenta  0% * Humedad  48% * Presión  1014 hPa * Amanecer  08:00 * Anochecer  17:54 * Fase Lunar  Luna Nueva | Despejado 14° 26 de JUN Mañana a las 14:00   * Viento  11   km/h * Ráfagas  17   km/h * Sensación térmica  13° * Lluvia  0 mm * Nieve  0   cm * Nubes [...] | 20:00 | Intervalos nubosos 9° 25 de JUN Hoy a las 20:00   * Viento  8   km/h * Ráfagas  12   km/h * Sensación térmica  8° * Lluvia  0 mm * Nieve  0   cm * Nubes  73% * Prob. de precipitación  20% * Radiación UV  2 Baja * Prob. de Tormenta  0% * Humedad  67% * Presión  1015 hPa * Amanecer  08:00 * Anochecer  17:54 * Fase Lunar  Luna Nueva | Despejado 8° 26 de JUN Mañana a las 20:00   * Viento  9   km/h * Ráfagas  13   km/h * Sensación térmica  7° * Lluvia  0 mm * Nieve  0   cm * Nubes  0% *', 'score': 0.57236487}, {'title': 'Tiempo en Buenos Aires por horas', 'url': 'https://www.meteored.com.ar/tiempo-en_Buenos+Aires-America+Sur-Argentina-Ciudad+Autonoma+de+Buenos+Aires-SABE-horas-13584.html', 'content': 'La previsión del tiempo por horas en Buenos Aires ; 10:00, Soleado, 8° ; 11:00, Soleado, 9° ; 12:00, Soleado, 10° ; 13:00, Nubes y claros, 12° ; 14:00, Nubes y', 'score': 0.5709301}]\u001b[0m\u001b[32;1m\u001b[1;3mLa temperatura en Buenos Aires, Argentina es de 12°C y la sensación térmica es de 13°C. El cielo está parcialmente nuboso.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Puedes usar la herramienta tavily para buscar cual es la temperatura en buenos aires argentina?',\n",
       " 'output': 'La temperatura en Buenos Aires, Argentina es de 12°C y la sensación térmica es de 13°C. El cielo está parcialmente nuboso.\\n'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando el servicio de search\n",
    "agent_executor.invoke({\"input\": \"Puedes usar la herramienta tavily para buscar cual es la temperatura en buenos aires argentina?\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7594800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3199020f",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8949c",
   "metadata": {},
   "source": [
    "**RAG**: Retrieval Augmented Generation\n",
    "Componentes:\n",
    "- *chunks*: fragmentos de texto en los que se divide el archivo a procesar\n",
    "- *embeddings*: representacion numerica de lo que contiene cada chunk\n",
    "- *Chroma*: base de datos vectorial diseñada específicamente para almacenar, gestionar y buscar embeddings\n",
    "- *consulta*: se vuelve una representacion numerica\n",
    "- *retrieval*: la consulta se compara con los embeddings y se obtienen los chunks relevantes\n",
    "\n",
    "Doc gemini embeddings model: https://ai.google.dev/gemini-api/docs/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797a69c",
   "metadata": {},
   "source": [
    "**Primer paso - Preparar los datos**\n",
    "- generar chunks\n",
    "- generar embebes y almacenarlos en la base de datos Chroma\n",
    "\n",
    "\n",
    "Se realiza en archivo RAG_lab1 ya que el guardar los embebebs en la bd tiene cirta incompatibilidad con jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bd7f6",
   "metadata": {},
   "source": [
    "**Segundo paso - Consultar**\n",
    "- carga la base de datos\n",
    "- compara a query contra los chunks\n",
    "- obtiene los top x mas parecidos\n",
    "\n",
    "\n",
    "Se realiza en archivo RAG_lab1b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1369684",
   "metadata": {},
   "source": [
    "# Ecosistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079eed49",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc38a2a",
   "metadata": {},
   "source": [
    "**LangGraph**\n",
    "Es una biblioteca para construir app multifactor y con estado utilizando llm\n",
    "- Puede cordinar multiples chain o agents a travea sw multiples etpadas\n",
    "- Puede implementar comportamientos, tomar decisiones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681258d",
   "metadata": {},
   "source": [
    "## LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc97ee",
   "metadata": {},
   "source": [
    "**LangSmith**: Plataforma unificada para depurar, probar y monitorear aplicaciones de modelos de lenguejas y agentes inteligentes\n",
    "- gratis para un unico usuario\n",
    "- https://www.langchain.com/pricing-langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f96597",
   "metadata": {},
   "source": [
    "## Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d654fb3",
   "metadata": {},
   "source": [
    "**Hub** Prompts de la comunidad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda1c00",
   "metadata": {},
   "source": [
    "## LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6acc15",
   "metadata": {},
   "source": [
    "**LangServe**: Facilita el despliegue y mantenimiento de app, convierte apps de ml en servidores API\n",
    "- construido con FastAPI\n",
    "- Se registran todas las entradas y salidas del servidor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d89f9",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1df81",
   "metadata": {},
   "source": [
    "## Antrophic Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0f64c",
   "metadata": {},
   "source": [
    "Anthropic models: \n",
    "- https://docs.anthropic.com/en/docs/models-overview \n",
    "- Pago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006ae9d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m promt = ChatPromptTemplate.from_messages(\n\u001b[32m      5\u001b[39m     [\n\u001b[32m      6\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m      8\u001b[39m     ]\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m chain = promt | model\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTell me a funny joke about Python.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1457\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1455\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._create(payload)\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1457\u001b[39m     \u001b[43m_handle_anthropic_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_output(data, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1455\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1453\u001b[39m payload = \u001b[38;5;28mself\u001b[39m._get_request_payload(messages, stop=stop, **kwargs)\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1455\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1457\u001b[39m     _handle_anthropic_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1316\u001b[39m, in \u001b[36mChatAnthropic._create\u001b[39m\u001b[34m(self, payload)\u001b[39m\n\u001b[32m   1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.beta.messages.create(**payload)\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\anthropic\\_utils\\_utils.py:283\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\anthropic\\resources\\messages\\messages.py:978\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    972\u001b[39m     warnings.warn(\n\u001b[32m    973\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    974\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    975\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\anthropic\\_base_client.py:1307\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1293\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1294\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1295\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1302\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1303\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1304\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1305\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1306\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\anthropic\\_base_client.py:1102\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1099\u001b[39m             err.response.read()\n\u001b[32m   1101\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-opus-latest\")\n",
    "promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = promt | model\n",
    "chain.invoke({\"input\": \"Tell me a funny joke about Python.\"}).content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eacd1c5",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66f00c",
   "metadata": {},
   "source": [
    "- Pago\n",
    "- Chat Model Documents: https://python.langchain.com/v0.2/docs/integrations/chat/\n",
    "- OpenAI Chat Model Documents: https://python.langchain.com/v0.2/docs/integrations/chat/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30603cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = promt | model\n",
    "chain.invoke({\"input\": \"Tell me a funny joke about Python.\"}).content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee257b84",
   "metadata": {},
   "source": [
    "## Google - Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25756e56",
   "metadata": {},
   "source": [
    "- Gratuito con limitaciones \n",
    "- https://console.cloud.google.com/gen-app-builder/engines\n",
    "- https://ai.google.dev/gemini-api/docs/models/gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f05d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the Python say goodbye to the C++ programmer?\\n\\nBecause he didn't want to get bitten by a class!\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = promt | llm\n",
    "chain.invoke({\"input\": \"Tell me a funny joke about Python.\"}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0fb2c",
   "metadata": {},
   "source": [
    "## IBM Granite, Flan-T5, Mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f153adf",
   "metadata": {},
   "source": [
    "- paquete Python langchain-ibm. \n",
    "- Clases como WatsonxLLM y ChatWatsonx te permiten incorporar los modelos fundacionales de IBM (como los modelos Granite, Flan-T5, Mixtral, etc.) \n",
    "- Modelos de Embeddings: WatsonxEmbeddings te permite generar embeddings de texto usando los modelos de IBM, lo cual es crucial para aplicaciones de RAG (Generación Aumentada por Recuperación) y búsqueda semántica.\n",
    "Rerankers: WatsonxRerank ofrece capacidades para reordenar los resultados de búsqueda, mejorando la relevancia en los flujos de trabajo de RAG.\n",
    "- Toolkits: WatsonxToolkit proporciona un conjunto de herramientas diseñadas específicamente para interactuar con los servicios de watsonx dentro de los agentes de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20aee29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Puedes incluir variables como {input}, {history}, {tools}\n",
    "custom_prompt2 = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente muy útil. Usa tus herramientas, en cuanto tengas una respuesta lo comunicas y finalizas.\n",
    "\n",
    "Historial de conversación:\n",
    "{chat_history}\n",
    "\n",
    "Herramientas disponibles: {tools}\n",
    "                                             {agent_scratchpad}\n",
    "                                             {tool_names}\n",
    "\n",
    "Usuario: {input}\n",
    "Asistente:\n",
    "\"\"\")\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"         \n",
    "     Eres un asistente de IA muy útil y amigable. Responde siempre de forma clara y concisa.        \n",
    "      Utiliza las herramientas disponibles para obtener la información necesaria.Herramientas disponibles:  {tool_names} {tools}\n",
    "     Historial de conversación: {chat_history}   \n",
    "     {agent_scratchpad}    \n",
    "     Cuando tengas la respuesta final a la pregunta del usuario o hayas completado la tarea,         \n",
    "     responde usando el siguiente formato especial:         \n",
    "     Final Answer: [Tu respuesta final aquí]         \"\"\"),         \n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),         \n",
    "    (\"human\", \"{input}\"),         \n",
    "    ])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ae19126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "def sumar(a, b):\n",
    "    return str(int(a) + int(b))\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Sumador\",\n",
    "        func=lambda x: sumar(*x.split()),\n",
    "        description=\"Suma dos números dados separados por un espacio. Ejemplo: '3 4'\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "05e0b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2f9955df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    prompt= custom_prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory = memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7094bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: 8\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '¿Puedes sumar 3 5?', 'chat_history': [HumanMessage(content='¿Puedes sumar 3 5?', additional_kwargs={}, response_metadata={}), AIMessage(content='8', additional_kwargs={}, response_metadata={}), HumanMessage(content='¿Puedes sumar 3 5?', additional_kwargs={}, response_metadata={}), AIMessage(content='8', additional_kwargs={}, response_metadata={})], 'output': '8', 'intermediate_steps': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Socius\\Desktop\\Langchain\\langchain practica\\.venv\\Lib\\site-packages\\langchain\\memory\\chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "respuesta = agent_executor.invoke(\n",
    "                    {\"input\": \"¿Puedes sumar 3 5?\"}\n",
    "                )\n",
    "\n",
    "print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
